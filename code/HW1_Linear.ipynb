{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4oKbqMWMpMN"
      },
      "source": [
        "# CS1470/2470 HW1: Single-Layered Neural Networks\n",
        "\n",
        "In this homework assignment, you will build a simple linear model using differential modules.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CLyvlysHCK9"
      },
      "source": [
        "## Starting Our Modular API\n",
        "\n",
        "### **The goal of this assignment is as follows:** \n",
        "- Organize our understanding of deep learning into a modular framework. \n",
        "- Get familiarized with a simple modular API which reflects (but is a simplification of) some of PyTorch's systems. \n",
        "- Implement some nice modular components and be able to construct a functional single-file neural network from it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUeG0msHRyo"
      },
      "source": [
        "### Improving The Old Way via Chain Rule\n",
        "\n",
        "Recall that in the machine learning lab, we got the chance to make some simple but effective regression models! Given some realizations $X \\sim \\mathcal{X}$ and $Y = \\mathbb{E}[Y|X] + \\xi$, we were able to train up a model $h_{\\theta} \\in \\mathcal{H}$ which was similar to $\\mathbb{E}[Y|X]$ and thereby minimized an empirical loss $\\mathcal{L}$ of our choice.\n",
        "\n",
        "> As a reminder, $\\mathcal{X}$ is the space of all candidate functions.\n",
        "\n",
        "In these cases, we assumed that $h_{\\theta}$ had a relatively simple and non-flexible architecture, which meant that we could manually specify the structure and simply derive and code up our gradient formula once. Furthermore, since the optimization process was concave, we could even skip the gradient computation and directly derive a loss-minimizing parameter selection. \n",
        "\n",
        "Of course, there are hard limits to what this kind of architecture can provide us; sometimes the relationships that the model needs to capture are relatively complex and might not be resolvable in such a fashion. And that's why this course exists!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmnuKJmCHmpg"
      },
      "source": [
        "#### **Problem:** \n",
        "> This is extremely time-consuming and rigid! What happens if we switched out an activation function? A loss function? We'd have to re-specify the gradient every time!\n",
        "\n",
        "#### **Solution:** \n",
        "> Let's take advantage of the chain rule! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecdHbe0xHb1o"
      },
      "source": [
        "**Naive Solution:** If we really wanted to, we could approach this problem in the same way as before, and just code up the gradient functions manually. Similarly to before, this would allow us to propagate gradients through, say, a specified loss function, an activation function, and a dense layer. We could also do it for 2 dense layers; just use the old gradient function for the weights in layer 2, compute the new gradient for the weights in layer 1, and so on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTfm_9fmHkhE"
      },
      "source": [
        "Recall that per the chain rule, if there exists a set of differentiable functions $c(b)$ and $b(a)$, then \n",
        "\n",
        "$$\\frac{\\partial a}{\\partial c} = \\frac{\\partial a}{\\partial b} \\frac{\\partial b}{\\partial c}$$\n",
        "\n",
        "Going back to our regression model, let's assume that we have a layered process: \n",
        "\n",
        "$$x \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
        "\n",
        "This implies that we can compute the partial of the trainable parameters $\\theta$ through a loss evaluation $\\mathcal{L}$ and a dense layer $h_\\theta(x)$ by the following relationship:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{h_\\theta}}{\\partial \\theta}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
        "\n",
        "With a similar logic, you can also make the assertion that you can also get the partial with respect to the input $x$:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h_\\theta}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
        "\n",
        "So... by the same token, is there anything stopping us from going further? Let's say that we decided to have another hypothesis function such that $x = h'_{\\theta'}(x')$ for some other hypothesis function and inputs? The new structure would then be: \n",
        "\n",
        "$$x' \\to \\big[ x = h'_{\\theta'}(x') \\big] \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
        "\n",
        "Without the chain rule, coding in the facilities to optimize $\\theta'$ might have been tricky, but with the chain rule we know that:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial h}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h} \n",
        "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial \\mathcal{L}}{\\partial x}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUJfaOFpIJbD"
      },
      "source": [
        "Notice how this process is both predictable and scales very well! Say that we wanted to add some activation functions to restrict the range of the hypothesis functions. This trivially inserts into the chain and everything still works and will look something like this: \n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h}{\\partial x}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "\\ \\text{ and } \\ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial h}{\\partial \\theta}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVyTXT5TIOsz"
      },
      "source": [
        "And with that, we start to approach the reason why this is such a powerful formulation: The cumulative nature of the process. Specifically, consider the process that needs to happen in order to compute this for the extended 2-layer example: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "= \\frac{\\partial \\mathcal{L}}{\\partial h} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'}\n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
        "\\\\\n",
        "&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
        "&&&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "% \\\\\n",
        "% &\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'} \n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
        "% &\n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "% \\\\ \n",
        "% &&\n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "... and this is the process known as **back-propagation** *(and a special case of **auto-differentiation**)*!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdK16iXOVgta"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhgMs3u_S1OU"
      },
      "source": [
        "## Loading In Our Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV7vjVbTf9Xg"
      },
      "source": [
        "The first thing we have to do is load in our data to use our model with. We'll be working with the [diabetes dataset from the sklearn package](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). We've already provided the code to load in the input data and the ground truth labels (stored as `X` and `Y` respectively below).\n",
        "\n",
        "**[TODO]:** Split the samples into training and testing sets. We'll train with the train set and reserve the testing set to evaluate the model's performance on samples it hasn't seen.\n",
        "\n",
        "The diabetes dataset has 442 samples. Each sample's input data has 10 data points for some key metrics, like age and cholesterol levels. The \"label\" is a number representing disease progression one year after baseline. Thus, `X` has shape `(442, 10)`, while `Y` has shape `(442,)`.\n",
        "\n",
        "**[TODO]:** Reshape the `Y` subsets to have shape `(num_samples, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gytrDuxV5gFO",
        "outputId": "ac0b50dd-5e70-4318-dc87-6a1651b43f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Input shape: (353, 10) for training, (89, 10) for testing\n",
            "> Label shape: (353, 1) for training, (89, 1) for testing\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X, Y = diabetes.data, diabetes.target\n",
        "\n",
        "## TODO: Split the data into a 80%-20% training-testing split\n",
        "## TODO: Reshape the Y subsets to have shape (num_samples, 1)\n",
        "X0, X1, Y0, Y1 = X[:353,:], X[353:,:], Y[:353].reshape(353,1), Y[353:].reshape(89,1)\n",
        "\n",
        "print(f\"\"\"\n",
        "> Input shape: {X0.shape} for training, {X1.shape} for testing\n",
        "> Label shape: {Y0.shape} for training, {Y1.shape} for testing\n",
        "\"\"\".strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYU_639VgHq"
      },
      "source": [
        "## **Exploring a possible modular implementation: PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHs57_36jfDL"
      },
      "source": [
        "Next, we'll want to build up a Regression model interface, from which we can implement specific regression model classes, like the LinearRegression class we'll work with later. \n",
        "\n",
        "We subclass the `nn.Module class`, which represents any module (like a layer or model) of a deep learning system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "metadata": {
        "id": "DG6vkwv5chPg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Regression(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Initialize all the inherent \"things\" inside of a model!\n",
        "    This includes things like the layers, activation/loss functions, and optimzer. \n",
        "    \"\"\"\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")        \n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dims, output_dims).to(self.device)   \n",
        "        self.activation = None  ## To be specified in subclasses \n",
        "        self.loss = None        ## To be specified in subclasses \n",
        "        self.set_learning_rate()\n",
        "\n",
        "    \"\"\"\n",
        "    Sets up the optmizer\n",
        "    \"\"\"\n",
        "    def set_learning_rate(self, learning_rate=0.001):\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate) ## Simple stochastic gradient descent (SGD) optimizer\n",
        "\n",
        "    \"\"\"\n",
        "    Forward pass of the model\n",
        "    Given an input x, how does the model process the input to get its output?\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTzEFlA0TFZB"
      },
      "source": [
        "## Adding a PyTorch Training/Evaluation Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSK640PoDWX"
      },
      "source": [
        "Now that we have the basis for a Regression model, we need to describe how to train (`fit`) and evaluate (`evaluate`) our model, given data. \n",
        "Every epoch, we want to fit our model to the training data, and then evaluate our model on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "metadata": {
        "id": "aUetVEGtS9k9"
      },
      "outputs": [],
      "source": [
        "class TrainTest:\n",
        "\n",
        "    no_grad = torch.no_grad\n",
        "\n",
        "    def fit(self, data):\n",
        "        ## Training loop\n",
        "        self.train()        ## Set model into training mode\n",
        "        ## Iterate over the data batches\n",
        "        for batch, (inputs, target) in enumerate(data):\n",
        "            ## In real pytorch, you'd need to set the device\n",
        "            inputs = inputs.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "            ## Erase the gradient history\n",
        "            self.optimizer.zero_grad()\n",
        "            ## Do a forward pass on the model\n",
        "            output = self(inputs)\n",
        "            ## Compute the loss\n",
        "            loss = self.loss(output, target)\n",
        "            ## Run backwards pass from the loss through the previous layers\n",
        "            ## This will accumulate gradients for the parameters that need to be optimized\n",
        "            loss.backward()\n",
        "            ## Perform a single optimization step\n",
        "            self.optimizer.step()\n",
        "        return {'loss' : loss}\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        ## Set model into \"evaluate\" mode so that the parameters don't get updated\n",
        "        self.eval()\n",
        "        total_loss = 0\n",
        "        ## Cut off the tensor training scope to make sure weights aren't updated\n",
        "        ## For now, it's torch.no_grad; later, you'll use Tensor.no_grad\n",
        "        with TrainTest.no_grad():\n",
        "            for inputs, target in data:\n",
        "                ## In real pytorch, you'd need to set the device\n",
        "                inputs = inputs.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "                output = self(inputs)\n",
        "                total_loss += self.loss(output, target).item()  # sum up batch loss\n",
        "\n",
        "        total_loss /= len(data)\n",
        "        return {'test_loss' : total_loss}\n",
        "        \n",
        "    def train_test(self, train_data, test_data, epochs=1):\n",
        "        ## Does both training and validation on a per-epoch basis\n",
        "        all_stats = []\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self.fit(train_data)\n",
        "            test_stats = self.evaluate(test_data)\n",
        "            all_stats += [{**train_stats, **test_stats}]\n",
        "            print(f'[Epoch {epoch+1}/{epochs}]', all_stats[-1])\n",
        "        return all_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNhzYnUTNMa"
      },
      "source": [
        "## Making Linear Regression with Train/Test Capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njv7q2UTTgQE"
      },
      "source": [
        "Next, we implement the `LinearRegression` class, which subclasses both `Regression` and `TrainTest` to inherit useful methods. \n",
        "\n",
        "Consider a dense layer represented by $Y = XA + B$ where: \n",
        "- $X$ has shape `(n,input_dims)`\n",
        "- $A$ has shape `(input_dims,output_dims)`\n",
        "- $B$ has shape `(n,)`\n",
        "- $Y$ has shape `(n, output_dims)`. \n",
        "\n",
        "Let each row of $X, B, Y$ (or the 0$^\\text{th}$ dimension) represent a different sample. Disregarding bias, note how any given row in $Y$ is a set of linear combinations of the same row in $X$. In other words, a Dense Layer is high-dimensional Linear Regression. \n",
        "\n",
        "Once you've built up your model, try training and testing it in the code block below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {
        "id": "gYU8NKSdTc4W"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(Regression, TrainTest):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__(input_dims, output_dims)\n",
        "        self.activation = nn.Identity()\n",
        "        self.loss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBUaTs3N_hmn",
        "outputId": "e8fe6442-ea59-4810-cc9f-3b2a556df079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/200] {'loss': tensor(28733.0488, grad_fn=<MseLossBackward0>), 'test_loss': 10470.96875}\n",
            "[Epoch 2/200] {'loss': tensor(9450.1357, grad_fn=<MseLossBackward0>), 'test_loss': 7129.34130859375}\n",
            "[Epoch 3/200] {'loss': tensor(6345.9321, grad_fn=<MseLossBackward0>), 'test_loss': 6518.35302734375}\n",
            "[Epoch 4/200] {'loss': tensor(5830.4917, grad_fn=<MseLossBackward0>), 'test_loss': 6378.36572265625}\n",
            "[Epoch 5/200] {'loss': tensor(5729.4375, grad_fn=<MseLossBackward0>), 'test_loss': 6327.45068359375}\n",
            "[Epoch 6/200] {'loss': tensor(5694.8677, grad_fn=<MseLossBackward0>), 'test_loss': 6296.36669921875}\n",
            "[Epoch 7/200] {'loss': tensor(5671.1172, grad_fn=<MseLossBackward0>), 'test_loss': 6270.7880859375}\n",
            "[Epoch 8/200] {'loss': tensor(5649.2778, grad_fn=<MseLossBackward0>), 'test_loss': 6247.12646484375}\n",
            "[Epoch 9/200] {'loss': tensor(5627.9219, grad_fn=<MseLossBackward0>), 'test_loss': 6224.28857421875}\n",
            "[Epoch 10/200] {'loss': tensor(5606.8193, grad_fn=<MseLossBackward0>), 'test_loss': 6201.892578125}\n",
            "[Epoch 11/200] {'loss': tensor(5585.9316, grad_fn=<MseLossBackward0>), 'test_loss': 6179.79345703125}\n",
            "[Epoch 12/200] {'loss': tensor(5565.2505, grad_fn=<MseLossBackward0>), 'test_loss': 6157.9326171875}\n",
            "[Epoch 13/200] {'loss': tensor(5544.7734, grad_fn=<MseLossBackward0>), 'test_loss': 6136.28662109375}\n",
            "[Epoch 14/200] {'loss': tensor(5524.4971, grad_fn=<MseLossBackward0>), 'test_loss': 6114.84326171875}\n",
            "[Epoch 15/200] {'loss': tensor(5504.4194, grad_fn=<MseLossBackward0>), 'test_loss': 6093.599609375}\n",
            "[Epoch 16/200] {'loss': tensor(5484.5391, grad_fn=<MseLossBackward0>), 'test_loss': 6072.55126953125}\n",
            "[Epoch 17/200] {'loss': tensor(5464.8530, grad_fn=<MseLossBackward0>), 'test_loss': 6051.6943359375}\n",
            "[Epoch 18/200] {'loss': tensor(5445.3604, grad_fn=<MseLossBackward0>), 'test_loss': 6031.02734375}\n",
            "[Epoch 19/200] {'loss': tensor(5426.0581, grad_fn=<MseLossBackward0>), 'test_loss': 6010.54931640625}\n",
            "[Epoch 20/200] {'loss': tensor(5406.9448, grad_fn=<MseLossBackward0>), 'test_loss': 5990.25634765625}\n",
            "[Epoch 21/200] {'loss': tensor(5388.0176, grad_fn=<MseLossBackward0>), 'test_loss': 5970.1474609375}\n",
            "[Epoch 22/200] {'loss': tensor(5369.2744, grad_fn=<MseLossBackward0>), 'test_loss': 5950.2216796875}\n",
            "[Epoch 23/200] {'loss': tensor(5350.7144, grad_fn=<MseLossBackward0>), 'test_loss': 5930.47607421875}\n",
            "[Epoch 24/200] {'loss': tensor(5332.3350, grad_fn=<MseLossBackward0>), 'test_loss': 5910.90869140625}\n",
            "[Epoch 25/200] {'loss': tensor(5314.1348, grad_fn=<MseLossBackward0>), 'test_loss': 5891.51806640625}\n",
            "[Epoch 26/200] {'loss': tensor(5296.1104, grad_fn=<MseLossBackward0>), 'test_loss': 5872.30322265625}\n",
            "[Epoch 27/200] {'loss': tensor(5278.2612, grad_fn=<MseLossBackward0>), 'test_loss': 5853.26025390625}\n",
            "[Epoch 28/200] {'loss': tensor(5260.5859, grad_fn=<MseLossBackward0>), 'test_loss': 5834.38916015625}\n",
            "[Epoch 29/200] {'loss': tensor(5243.0811, grad_fn=<MseLossBackward0>), 'test_loss': 5815.68701171875}\n",
            "[Epoch 30/200] {'loss': tensor(5225.7461, grad_fn=<MseLossBackward0>), 'test_loss': 5797.15283203125}\n",
            "[Epoch 31/200] {'loss': tensor(5208.5786, grad_fn=<MseLossBackward0>), 'test_loss': 5778.7841796875}\n",
            "[Epoch 32/200] {'loss': tensor(5191.5771, grad_fn=<MseLossBackward0>), 'test_loss': 5760.58056640625}\n",
            "[Epoch 33/200] {'loss': tensor(5174.7388, grad_fn=<MseLossBackward0>), 'test_loss': 5742.54052734375}\n",
            "[Epoch 34/200] {'loss': tensor(5158.0635, grad_fn=<MseLossBackward0>), 'test_loss': 5724.66064453125}\n",
            "[Epoch 35/200] {'loss': tensor(5141.5488, grad_fn=<MseLossBackward0>), 'test_loss': 5706.9404296875}\n",
            "[Epoch 36/200] {'loss': tensor(5125.1929, grad_fn=<MseLossBackward0>), 'test_loss': 5689.376953125}\n",
            "[Epoch 37/200] {'loss': tensor(5108.9941, grad_fn=<MseLossBackward0>), 'test_loss': 5671.97021484375}\n",
            "[Epoch 38/200] {'loss': tensor(5092.9512, grad_fn=<MseLossBackward0>), 'test_loss': 5654.71826171875}\n",
            "[Epoch 39/200] {'loss': tensor(5077.0620, grad_fn=<MseLossBackward0>), 'test_loss': 5637.619140625}\n",
            "[Epoch 40/200] {'loss': tensor(5061.3247, grad_fn=<MseLossBackward0>), 'test_loss': 5620.671875}\n",
            "[Epoch 41/200] {'loss': tensor(5045.7378, grad_fn=<MseLossBackward0>), 'test_loss': 5603.87353515625}\n",
            "[Epoch 42/200] {'loss': tensor(5030.3003, grad_fn=<MseLossBackward0>), 'test_loss': 5587.22412109375}\n",
            "[Epoch 43/200] {'loss': tensor(5015.0103, grad_fn=<MseLossBackward0>), 'test_loss': 5570.72119140625}\n",
            "[Epoch 44/200] {'loss': tensor(4999.8652, grad_fn=<MseLossBackward0>), 'test_loss': 5554.36376953125}\n",
            "[Epoch 45/200] {'loss': tensor(4984.8657, grad_fn=<MseLossBackward0>), 'test_loss': 5538.150390625}\n",
            "[Epoch 46/200] {'loss': tensor(4970.0083, grad_fn=<MseLossBackward0>), 'test_loss': 5522.0791015625}\n",
            "[Epoch 47/200] {'loss': tensor(4955.2925, grad_fn=<MseLossBackward0>), 'test_loss': 5506.14794921875}\n",
            "[Epoch 48/200] {'loss': tensor(4940.7158, grad_fn=<MseLossBackward0>), 'test_loss': 5490.35791015625}\n",
            "[Epoch 49/200] {'loss': tensor(4926.2778, grad_fn=<MseLossBackward0>), 'test_loss': 5474.705078125}\n",
            "[Epoch 50/200] {'loss': tensor(4911.9775, grad_fn=<MseLossBackward0>), 'test_loss': 5459.189453125}\n",
            "[Epoch 51/200] {'loss': tensor(4897.8110, grad_fn=<MseLossBackward0>), 'test_loss': 5443.80810546875}\n",
            "[Epoch 52/200] {'loss': tensor(4883.7798, grad_fn=<MseLossBackward0>), 'test_loss': 5428.56201171875}\n",
            "[Epoch 53/200] {'loss': tensor(4869.8809, grad_fn=<MseLossBackward0>), 'test_loss': 5413.44677734375}\n",
            "[Epoch 54/200] {'loss': tensor(4856.1123, grad_fn=<MseLossBackward0>), 'test_loss': 5398.462890625}\n",
            "[Epoch 55/200] {'loss': tensor(4842.4736, grad_fn=<MseLossBackward0>), 'test_loss': 5383.60986328125}\n",
            "[Epoch 56/200] {'loss': tensor(4828.9639, grad_fn=<MseLossBackward0>), 'test_loss': 5368.884765625}\n",
            "[Epoch 57/200] {'loss': tensor(4815.5811, grad_fn=<MseLossBackward0>), 'test_loss': 5354.2880859375}\n",
            "[Epoch 58/200] {'loss': tensor(4802.3232, grad_fn=<MseLossBackward0>), 'test_loss': 5339.81640625}\n",
            "[Epoch 59/200] {'loss': tensor(4789.1904, grad_fn=<MseLossBackward0>), 'test_loss': 5325.46923828125}\n",
            "[Epoch 60/200] {'loss': tensor(4776.1807, grad_fn=<MseLossBackward0>), 'test_loss': 5311.24560546875}\n",
            "[Epoch 61/200] {'loss': tensor(4763.2920, grad_fn=<MseLossBackward0>), 'test_loss': 5297.14453125}\n",
            "[Epoch 62/200] {'loss': tensor(4750.5239, grad_fn=<MseLossBackward0>), 'test_loss': 5283.1640625}\n",
            "[Epoch 63/200] {'loss': tensor(4737.8755, grad_fn=<MseLossBackward0>), 'test_loss': 5269.30419921875}\n",
            "[Epoch 64/200] {'loss': tensor(4725.3447, grad_fn=<MseLossBackward0>), 'test_loss': 5255.56201171875}\n",
            "[Epoch 65/200] {'loss': tensor(4712.9302, grad_fn=<MseLossBackward0>), 'test_loss': 5241.9375}\n",
            "[Epoch 66/200] {'loss': tensor(4700.6323, grad_fn=<MseLossBackward0>), 'test_loss': 5228.42822265625}\n",
            "[Epoch 67/200] {'loss': tensor(4688.4473, grad_fn=<MseLossBackward0>), 'test_loss': 5215.03564453125}\n",
            "[Epoch 68/200] {'loss': tensor(4676.3765, grad_fn=<MseLossBackward0>), 'test_loss': 5201.755859375}\n",
            "[Epoch 69/200] {'loss': tensor(4664.4170, grad_fn=<MseLossBackward0>), 'test_loss': 5188.58935546875}\n",
            "[Epoch 70/200] {'loss': tensor(4652.5674, grad_fn=<MseLossBackward0>), 'test_loss': 5175.53369140625}\n",
            "[Epoch 71/200] {'loss': tensor(4640.8281, grad_fn=<MseLossBackward0>), 'test_loss': 5162.58935546875}\n",
            "[Epoch 72/200] {'loss': tensor(4629.1968, grad_fn=<MseLossBackward0>), 'test_loss': 5149.7529296875}\n",
            "[Epoch 73/200] {'loss': tensor(4617.6733, grad_fn=<MseLossBackward0>), 'test_loss': 5137.02587890625}\n",
            "[Epoch 74/200] {'loss': tensor(4606.2554, grad_fn=<MseLossBackward0>), 'test_loss': 5124.40576171875}\n",
            "[Epoch 75/200] {'loss': tensor(4594.9429, grad_fn=<MseLossBackward0>), 'test_loss': 5111.8916015625}\n",
            "[Epoch 76/200] {'loss': tensor(4583.7339, grad_fn=<MseLossBackward0>), 'test_loss': 5099.482421875}\n",
            "[Epoch 77/200] {'loss': tensor(4572.6274, grad_fn=<MseLossBackward0>), 'test_loss': 5087.177734375}\n",
            "[Epoch 78/200] {'loss': tensor(4561.6230, grad_fn=<MseLossBackward0>), 'test_loss': 5074.9755859375}\n",
            "[Epoch 79/200] {'loss': tensor(4550.7197, grad_fn=<MseLossBackward0>), 'test_loss': 5062.875}\n",
            "[Epoch 80/200] {'loss': tensor(4539.9155, grad_fn=<MseLossBackward0>), 'test_loss': 5050.8759765625}\n",
            "[Epoch 81/200] {'loss': tensor(4529.2100, grad_fn=<MseLossBackward0>), 'test_loss': 5038.97607421875}\n",
            "[Epoch 82/200] {'loss': tensor(4518.6011, grad_fn=<MseLossBackward0>), 'test_loss': 5027.17626953125}\n",
            "[Epoch 83/200] {'loss': tensor(4508.0894, grad_fn=<MseLossBackward0>), 'test_loss': 5015.4736328125}\n",
            "[Epoch 84/200] {'loss': tensor(4497.6733, grad_fn=<MseLossBackward0>), 'test_loss': 5003.8671875}\n",
            "[Epoch 85/200] {'loss': tensor(4487.3521, grad_fn=<MseLossBackward0>), 'test_loss': 4992.3583984375}\n",
            "[Epoch 86/200] {'loss': tensor(4477.1230, grad_fn=<MseLossBackward0>), 'test_loss': 4980.94384765625}\n",
            "[Epoch 87/200] {'loss': tensor(4466.9878, grad_fn=<MseLossBackward0>), 'test_loss': 4969.62353515625}\n",
            "[Epoch 88/200] {'loss': tensor(4456.9438, grad_fn=<MseLossBackward0>), 'test_loss': 4958.39599609375}\n",
            "[Epoch 89/200] {'loss': tensor(4446.9902, grad_fn=<MseLossBackward0>), 'test_loss': 4947.2607421875}\n",
            "[Epoch 90/200] {'loss': tensor(4437.1265, grad_fn=<MseLossBackward0>), 'test_loss': 4936.216796875}\n",
            "[Epoch 91/200] {'loss': tensor(4427.3511, grad_fn=<MseLossBackward0>), 'test_loss': 4925.26416015625}\n",
            "[Epoch 92/200] {'loss': tensor(4417.6636, grad_fn=<MseLossBackward0>), 'test_loss': 4914.3994140625}\n",
            "[Epoch 93/200] {'loss': tensor(4408.0630, grad_fn=<MseLossBackward0>), 'test_loss': 4903.62451171875}\n",
            "[Epoch 94/200] {'loss': tensor(4398.5483, grad_fn=<MseLossBackward0>), 'test_loss': 4892.93701171875}\n",
            "[Epoch 95/200] {'loss': tensor(4389.1191, grad_fn=<MseLossBackward0>), 'test_loss': 4882.3359375}\n",
            "[Epoch 96/200] {'loss': tensor(4379.7734, grad_fn=<MseLossBackward0>), 'test_loss': 4871.82177734375}\n",
            "[Epoch 97/200] {'loss': tensor(4370.5117, grad_fn=<MseLossBackward0>), 'test_loss': 4861.39208984375}\n",
            "[Epoch 98/200] {'loss': tensor(4361.3320, grad_fn=<MseLossBackward0>), 'test_loss': 4851.046875}\n",
            "[Epoch 99/200] {'loss': tensor(4352.2339, grad_fn=<MseLossBackward0>), 'test_loss': 4840.78564453125}\n",
            "[Epoch 100/200] {'loss': tensor(4343.2168, grad_fn=<MseLossBackward0>), 'test_loss': 4830.60693359375}\n",
            "[Epoch 101/200] {'loss': tensor(4334.2788, grad_fn=<MseLossBackward0>), 'test_loss': 4820.509765625}\n",
            "[Epoch 102/200] {'loss': tensor(4325.4214, grad_fn=<MseLossBackward0>), 'test_loss': 4810.494140625}\n",
            "[Epoch 103/200] {'loss': tensor(4316.6416, grad_fn=<MseLossBackward0>), 'test_loss': 4800.55859375}\n",
            "[Epoch 104/200] {'loss': tensor(4307.9395, grad_fn=<MseLossBackward0>), 'test_loss': 4790.70263671875}\n",
            "[Epoch 105/200] {'loss': tensor(4299.3130, grad_fn=<MseLossBackward0>), 'test_loss': 4780.92529296875}\n",
            "[Epoch 106/200] {'loss': tensor(4290.7632, grad_fn=<MseLossBackward0>), 'test_loss': 4771.2255859375}\n",
            "[Epoch 107/200] {'loss': tensor(4282.2881, grad_fn=<MseLossBackward0>), 'test_loss': 4761.60400390625}\n",
            "[Epoch 108/200] {'loss': tensor(4273.8877, grad_fn=<MseLossBackward0>), 'test_loss': 4752.05810546875}\n",
            "[Epoch 109/200] {'loss': tensor(4265.5605, grad_fn=<MseLossBackward0>), 'test_loss': 4742.587890625}\n",
            "[Epoch 110/200] {'loss': tensor(4257.3062, grad_fn=<MseLossBackward0>), 'test_loss': 4733.19287109375}\n",
            "[Epoch 111/200] {'loss': tensor(4249.1245, grad_fn=<MseLossBackward0>), 'test_loss': 4723.8720703125}\n",
            "[Epoch 112/200] {'loss': tensor(4241.0132, grad_fn=<MseLossBackward0>), 'test_loss': 4714.62451171875}\n",
            "[Epoch 113/200] {'loss': tensor(4232.9722, grad_fn=<MseLossBackward0>), 'test_loss': 4705.4501953125}\n",
            "[Epoch 114/200] {'loss': tensor(4225.0020, grad_fn=<MseLossBackward0>), 'test_loss': 4696.34716796875}\n",
            "[Epoch 115/200] {'loss': tensor(4217.1001, grad_fn=<MseLossBackward0>), 'test_loss': 4687.31689453125}\n",
            "[Epoch 116/200] {'loss': tensor(4209.2671, grad_fn=<MseLossBackward0>), 'test_loss': 4678.3564453125}\n",
            "[Epoch 117/200] {'loss': tensor(4201.5015, grad_fn=<MseLossBackward0>), 'test_loss': 4669.4658203125}\n",
            "[Epoch 118/200] {'loss': tensor(4193.8027, grad_fn=<MseLossBackward0>), 'test_loss': 4660.64501953125}\n",
            "[Epoch 119/200] {'loss': tensor(4186.1709, grad_fn=<MseLossBackward0>), 'test_loss': 4651.892578125}\n",
            "[Epoch 120/200] {'loss': tensor(4178.6040, grad_fn=<MseLossBackward0>), 'test_loss': 4643.2080078125}\n",
            "[Epoch 121/200] {'loss': tensor(4171.1021, grad_fn=<MseLossBackward0>), 'test_loss': 4634.59033203125}\n",
            "[Epoch 122/200] {'loss': tensor(4163.6646, grad_fn=<MseLossBackward0>), 'test_loss': 4626.04052734375}\n",
            "[Epoch 123/200] {'loss': tensor(4156.2910, grad_fn=<MseLossBackward0>), 'test_loss': 4617.55615234375}\n",
            "[Epoch 124/200] {'loss': tensor(4148.9800, grad_fn=<MseLossBackward0>), 'test_loss': 4609.13720703125}\n",
            "[Epoch 125/200] {'loss': tensor(4141.7314, grad_fn=<MseLossBackward0>), 'test_loss': 4600.78271484375}\n",
            "[Epoch 126/200] {'loss': tensor(4134.5444, grad_fn=<MseLossBackward0>), 'test_loss': 4592.49267578125}\n",
            "[Epoch 127/200] {'loss': tensor(4127.4189, grad_fn=<MseLossBackward0>), 'test_loss': 4584.26611328125}\n",
            "[Epoch 128/200] {'loss': tensor(4120.3530, grad_fn=<MseLossBackward0>), 'test_loss': 4576.10302734375}\n",
            "[Epoch 129/200] {'loss': tensor(4113.3472, grad_fn=<MseLossBackward0>), 'test_loss': 4568.00146484375}\n",
            "[Epoch 130/200] {'loss': tensor(4106.4009, grad_fn=<MseLossBackward0>), 'test_loss': 4559.9619140625}\n",
            "[Epoch 131/200] {'loss': tensor(4099.5132, grad_fn=<MseLossBackward0>), 'test_loss': 4551.98388671875}\n",
            "[Epoch 132/200] {'loss': tensor(4092.6833, grad_fn=<MseLossBackward0>), 'test_loss': 4544.0654296875}\n",
            "[Epoch 133/200] {'loss': tensor(4085.9111, grad_fn=<MseLossBackward0>), 'test_loss': 4536.2080078125}\n",
            "[Epoch 134/200] {'loss': tensor(4079.1951, grad_fn=<MseLossBackward0>), 'test_loss': 4528.4091796875}\n",
            "[Epoch 135/200] {'loss': tensor(4072.5354, grad_fn=<MseLossBackward0>), 'test_loss': 4520.66943359375}\n",
            "[Epoch 136/200] {'loss': tensor(4065.9321, grad_fn=<MseLossBackward0>), 'test_loss': 4512.98828125}\n",
            "[Epoch 137/200] {'loss': tensor(4059.3835, grad_fn=<MseLossBackward0>), 'test_loss': 4505.3642578125}\n",
            "[Epoch 138/200] {'loss': tensor(4052.8892, grad_fn=<MseLossBackward0>), 'test_loss': 4497.79833984375}\n",
            "[Epoch 139/200] {'loss': tensor(4046.4495, grad_fn=<MseLossBackward0>), 'test_loss': 4490.2880859375}\n",
            "[Epoch 140/200] {'loss': tensor(4040.0620, grad_fn=<MseLossBackward0>), 'test_loss': 4482.83447265625}\n",
            "[Epoch 141/200] {'loss': tensor(4033.7285, grad_fn=<MseLossBackward0>), 'test_loss': 4475.435546875}\n",
            "[Epoch 142/200] {'loss': tensor(4027.4468, grad_fn=<MseLossBackward0>), 'test_loss': 4468.09326171875}\n",
            "[Epoch 143/200] {'loss': tensor(4021.2175, grad_fn=<MseLossBackward0>), 'test_loss': 4460.80419921875}\n",
            "[Epoch 144/200] {'loss': tensor(4015.0391, grad_fn=<MseLossBackward0>), 'test_loss': 4453.5693359375}\n",
            "[Epoch 145/200] {'loss': tensor(4008.9114, grad_fn=<MseLossBackward0>), 'test_loss': 4446.38818359375}\n",
            "[Epoch 146/200] {'loss': tensor(4002.8342, grad_fn=<MseLossBackward0>), 'test_loss': 4439.26025390625}\n",
            "[Epoch 147/200] {'loss': tensor(3996.8066, grad_fn=<MseLossBackward0>), 'test_loss': 4432.1845703125}\n",
            "[Epoch 148/200] {'loss': tensor(3990.8284, grad_fn=<MseLossBackward0>), 'test_loss': 4425.16064453125}\n",
            "[Epoch 149/200] {'loss': tensor(3984.8984, grad_fn=<MseLossBackward0>), 'test_loss': 4418.18798828125}\n",
            "[Epoch 150/200] {'loss': tensor(3979.0178, grad_fn=<MseLossBackward0>), 'test_loss': 4411.26708984375}\n",
            "[Epoch 151/200] {'loss': tensor(3973.1838, grad_fn=<MseLossBackward0>), 'test_loss': 4404.39599609375}\n",
            "[Epoch 152/200] {'loss': tensor(3967.3977, grad_fn=<MseLossBackward0>), 'test_loss': 4397.57568359375}\n",
            "[Epoch 153/200] {'loss': tensor(3961.6582, grad_fn=<MseLossBackward0>), 'test_loss': 4390.80419921875}\n",
            "[Epoch 154/200] {'loss': tensor(3955.9653, grad_fn=<MseLossBackward0>), 'test_loss': 4384.08203125}\n",
            "[Epoch 155/200] {'loss': tensor(3950.3184, grad_fn=<MseLossBackward0>), 'test_loss': 4377.408203125}\n",
            "[Epoch 156/200] {'loss': tensor(3944.7168, grad_fn=<MseLossBackward0>), 'test_loss': 4370.783203125}\n",
            "[Epoch 157/200] {'loss': tensor(3939.1602, grad_fn=<MseLossBackward0>), 'test_loss': 4364.2060546875}\n",
            "[Epoch 158/200] {'loss': tensor(3933.6477, grad_fn=<MseLossBackward0>), 'test_loss': 4357.67626953125}\n",
            "[Epoch 159/200] {'loss': tensor(3928.1794, grad_fn=<MseLossBackward0>), 'test_loss': 4351.1923828125}\n",
            "[Epoch 160/200] {'loss': tensor(3922.7554, grad_fn=<MseLossBackward0>), 'test_loss': 4344.755859375}\n",
            "[Epoch 161/200] {'loss': tensor(3917.3743, grad_fn=<MseLossBackward0>), 'test_loss': 4338.3642578125}\n",
            "[Epoch 162/200] {'loss': tensor(3912.0352, grad_fn=<MseLossBackward0>), 'test_loss': 4332.01953125}\n",
            "[Epoch 163/200] {'loss': tensor(3906.7390, grad_fn=<MseLossBackward0>), 'test_loss': 4325.7197265625}\n",
            "[Epoch 164/200] {'loss': tensor(3901.4844, grad_fn=<MseLossBackward0>), 'test_loss': 4319.46435546875}\n",
            "[Epoch 165/200] {'loss': tensor(3896.2720, grad_fn=<MseLossBackward0>), 'test_loss': 4313.2529296875}\n",
            "[Epoch 166/200] {'loss': tensor(3891.1006, grad_fn=<MseLossBackward0>), 'test_loss': 4307.0859375}\n",
            "[Epoch 167/200] {'loss': tensor(3885.9695, grad_fn=<MseLossBackward0>), 'test_loss': 4300.9619140625}\n",
            "[Epoch 168/200] {'loss': tensor(3880.8789, grad_fn=<MseLossBackward0>), 'test_loss': 4294.88232421875}\n",
            "[Epoch 169/200] {'loss': tensor(3875.8279, grad_fn=<MseLossBackward0>), 'test_loss': 4288.84423828125}\n",
            "[Epoch 170/200] {'loss': tensor(3870.8162, grad_fn=<MseLossBackward0>), 'test_loss': 4282.84814453125}\n",
            "[Epoch 171/200] {'loss': tensor(3865.8438, grad_fn=<MseLossBackward0>), 'test_loss': 4276.8955078125}\n",
            "[Epoch 172/200] {'loss': tensor(3860.9104, grad_fn=<MseLossBackward0>), 'test_loss': 4270.98291015625}\n",
            "[Epoch 173/200] {'loss': tensor(3856.0151, grad_fn=<MseLossBackward0>), 'test_loss': 4265.1123046875}\n",
            "[Epoch 174/200] {'loss': tensor(3851.1572, grad_fn=<MseLossBackward0>), 'test_loss': 4259.2822265625}\n",
            "[Epoch 175/200] {'loss': tensor(3846.3381, grad_fn=<MseLossBackward0>), 'test_loss': 4253.4921875}\n",
            "[Epoch 176/200] {'loss': tensor(3841.5549, grad_fn=<MseLossBackward0>), 'test_loss': 4247.7431640625}\n",
            "[Epoch 177/200] {'loss': tensor(3836.8096, grad_fn=<MseLossBackward0>), 'test_loss': 4242.033203125}\n",
            "[Epoch 178/200] {'loss': tensor(3832.0999, grad_fn=<MseLossBackward0>), 'test_loss': 4236.36279296875}\n",
            "[Epoch 179/200] {'loss': tensor(3827.4268, grad_fn=<MseLossBackward0>), 'test_loss': 4230.73095703125}\n",
            "[Epoch 180/200] {'loss': tensor(3822.7893, grad_fn=<MseLossBackward0>), 'test_loss': 4225.1376953125}\n",
            "[Epoch 181/200] {'loss': tensor(3818.1865, grad_fn=<MseLossBackward0>), 'test_loss': 4219.5830078125}\n",
            "[Epoch 182/200] {'loss': tensor(3813.6196, grad_fn=<MseLossBackward0>), 'test_loss': 4214.06591796875}\n",
            "[Epoch 183/200] {'loss': tensor(3809.0879, grad_fn=<MseLossBackward0>), 'test_loss': 4208.5869140625}\n",
            "[Epoch 184/200] {'loss': tensor(3804.5896, grad_fn=<MseLossBackward0>), 'test_loss': 4203.14453125}\n",
            "[Epoch 185/200] {'loss': tensor(3800.1252, grad_fn=<MseLossBackward0>), 'test_loss': 4197.7392578125}\n",
            "[Epoch 186/200] {'loss': tensor(3795.6948, grad_fn=<MseLossBackward0>), 'test_loss': 4192.37060546875}\n",
            "[Epoch 187/200] {'loss': tensor(3791.2979, grad_fn=<MseLossBackward0>), 'test_loss': 4187.0380859375}\n",
            "[Epoch 188/200] {'loss': tensor(3786.9341, grad_fn=<MseLossBackward0>), 'test_loss': 4181.74169921875}\n",
            "[Epoch 189/200] {'loss': tensor(3782.6023, grad_fn=<MseLossBackward0>), 'test_loss': 4176.48095703125}\n",
            "[Epoch 190/200] {'loss': tensor(3778.3037, grad_fn=<MseLossBackward0>), 'test_loss': 4171.2548828125}\n",
            "[Epoch 191/200] {'loss': tensor(3774.0369, grad_fn=<MseLossBackward0>), 'test_loss': 4166.0634765625}\n",
            "[Epoch 192/200] {'loss': tensor(3769.8018, grad_fn=<MseLossBackward0>), 'test_loss': 4160.908203125}\n",
            "[Epoch 193/200] {'loss': tensor(3765.5984, grad_fn=<MseLossBackward0>), 'test_loss': 4155.78564453125}\n",
            "[Epoch 194/200] {'loss': tensor(3761.4255, grad_fn=<MseLossBackward0>), 'test_loss': 4150.6982421875}\n",
            "[Epoch 195/200] {'loss': tensor(3757.2839, grad_fn=<MseLossBackward0>), 'test_loss': 4145.64453125}\n",
            "[Epoch 196/200] {'loss': tensor(3753.1731, grad_fn=<MseLossBackward0>), 'test_loss': 4140.623046875}\n",
            "[Epoch 197/200] {'loss': tensor(3749.0925, grad_fn=<MseLossBackward0>), 'test_loss': 4135.63525390625}\n",
            "[Epoch 198/200] {'loss': tensor(3745.0410, grad_fn=<MseLossBackward0>), 'test_loss': 4130.6806640625}\n",
            "[Epoch 199/200] {'loss': tensor(3741.0198, grad_fn=<MseLossBackward0>), 'test_loss': 4125.75830078125}\n",
            "[Epoch 200/200] {'loss': tensor(3737.0281, grad_fn=<MseLossBackward0>), 'test_loss': 4120.8681640625}\n"
          ]
        }
      ],
      "source": [
        "torch_model = LinearRegression(X0.shape[-1], 1)\n",
        "torch_model.set_learning_rate(0.3)\n",
        "torch_model.train_test(\n",
        "    [[torch.Tensor(X0), torch.Tensor(Y0)]], \n",
        "    [[torch.Tensor(X1), torch.Tensor(Y1)]],\n",
        "    epochs=200\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H63TERfyvZtS"
      },
      "source": [
        "## Shapes That Might Be Useful..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwbP5tRbJky"
      },
      "source": [
        "Throughout the duration of this course, you might find it really helpful to check the shapes of each of your different tensors and outputs just to verify that everything is working as intended. Check the block below to see an example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBkTktWcD9dz",
        "outputId": "2365cb9f-c19c-4acf-e18d-c8da60681996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Prediction Shape: torch.Size([353, 1])\n",
            "> Weights    Shape: torch.Size([1, 10])\n",
            "> Bias       Shape: torch.Size([1])\n",
            "> Loss       Shape: torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "y_true = torch.Tensor(Y0)\n",
        "y_pred = torch_model(torch.Tensor(X0))\n",
        "loss = torch_model.loss(y_true, y_pred)\n",
        "\n",
        "print(f\"\"\"\n",
        "> Prediction Shape: {y_pred.shape}\n",
        "> Weights    Shape: {list(torch_model.parameters())[0].shape}\n",
        "> Bias       Shape: {list(torch_model.parameters())[1].shape}\n",
        "> Loss       Shape: {loss.shape}\n",
        "\"\"\".strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu1pLK46fFqX"
      },
      "source": [
        "Next, let's start building up all the different parts of the basic PyTorch tools that we used in order to see what's under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZfozqWdLB2V"
      },
      "source": [
        "## PyTorch Complexity Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVv6iQios7-D"
      },
      "source": [
        "### Tensors\n",
        "- Tensors are responsible for maintaining their own gradients\n",
        "- Tensors hold on to `backward` functions to which they can pass a gradient into. These backwards functions are provided by the layers associated with those tensors. \n",
        "    - If a tensor is a terminal node, it will pass in an upstream gradient of `None`.\n",
        "    - If a tensor is a non-terminal node, it will pass the accumulated upstream gradient. \n",
        "    - `backward` functions as a linked list algorithm and crawls back the chain, computing the gradient for every tensor that it hits (as long as they require a gradient).\n",
        "- Since the tensors hold their own gradients, the optimizer can merely take the tensors' values, take their gradients, and then just optimize them.\n",
        "- However, because tensors are always keeping track of their gradients when `requires_grad` is set to `True`, we also want to add a way to stop tracking gradients.\n",
        "    - For instance, while evaluating the performance of our model, we want to make sure that the model doesn't learn anything from this evaluation phase.\n",
        "    - We can use the `no_grad` subclass to automatically handle the flipping of this `requires_grad` value. \n",
        "        - Every time we enter a `with Tensor.no_grad():` block, the code within `no_grad`'s `__enter__()` method will execute. \n",
        "        - Once we exit the same `with Tensor.no_grad():` block, `no_grad`'s `__exit()__` method will run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "5ZbMLm30y3tD"
      },
      "outputs": [],
      "source": [
        "class Tensor(np.ndarray):\n",
        "\n",
        "    '''\n",
        "    Subclassing numpy arrays is a bit weird:\n",
        "    https://numpy.org/doc/stable/user/basics.subclassing.html\n",
        "\n",
        "    Just assume that the attributes referred to in __new__/__array_finalize__ \n",
        "    will be accessible in a Tensor when a new Tensor object is created.  \n",
        "    '''\n",
        "\n",
        "    requires_grad = True  ## Class variable; accessible by Tensor.requires_grad\n",
        "\n",
        "    def __new__(cls, input_array):\n",
        "        obj = np.asarray(input_array).view(cls)\n",
        "        obj.backward = lambda x: None   ## Backward starts as None, gets assigned later\n",
        "        obj.grad = None                 ## Gradient starts as None, gets computed later\n",
        "        obj.requires_grad = True        ## By default, we'll want to compute gradient for new tensors\n",
        "        obj.to = lambda x: obj          ## We don't handle special device support (i.e. cpu vs gpu/cuda)\n",
        "        return obj\n",
        "\n",
        "    def __array_finalize__(self, obj):\n",
        "        if obj is None: return\n",
        "        self.backward       = getattr(obj, 'backward',      lambda x: None)\n",
        "        self.to             = getattr(obj, 'to',            lambda x: obj)\n",
        "        self.grad           = getattr(obj, 'grad',          None)\n",
        "        self.requires_grad  = getattr(obj, 'requires_grad', None)\n",
        "\n",
        "    class no_grad():\n",
        "\n",
        "        '''\n",
        "        Synergizes with Tensor: By entering the tensor with no_grad scope, \n",
        "        the Tensor.requires_grad singleton will swap to False. \n",
        "        '''\n",
        "        \n",
        "        def __enter__(self):\n",
        "            # When tape scope is entered, stop asking tensors to record gradients\n",
        "            Tensor.requires_grad = False\n",
        "            return self\n",
        "\n",
        "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "            # When tape scope is exited, let Diffable start recording to self.operation\n",
        "            Tensor.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovL7L6STvXCU"
      },
      "source": [
        "### Diffable\n",
        "\n",
        "Let's specify a \"Diffable\" object which will represent a module that can be differentiated. This class will make the following assumptions: \n",
        "- Gradients will need to flow through the input pathways in order to compute earlier gradients. \n",
        "    - Therefore, inputs will need an appropriate \"backward\"\n",
        "- Parameters will need to recieve gradients.\n",
        "- More specifically, if a `Diffable` object performs an operation on some input, then we know that the gradient from the output of the Diffable w.r.t. the inputs is the gradient of the `Diffable`'s operations w.r.t. its inputs.\n",
        "  - Thus, a `Diffable`'s `input_gradients()` function should return a tuple with each of the partial derivatives of the operations performed in the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {
        "id": "9v68mc_U1ymJ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod  # # For abstract method support\n",
        "\n",
        "class Diffable(ABC):\n",
        "    \"\"\"\n",
        "        We use these to represent differentiable layers which we can compute gradients for.\n",
        "    \"\"\"\n",
        "\n",
        "    def to(self, device):\n",
        "        return self         # Just there to ignore device setting calls\n",
        "    \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \n",
        "        ## The call method keeps track of method inputs and outputs\n",
        "        self.argnames   = self.forward.__code__.co_varnames[1:]\n",
        "        named_args      = {self.argnames[i] : args[i] for i in range(len(args))}\n",
        "        self.input_dict = {**named_args, **kwargs}\n",
        "        self.inputs     = [self.input_dict[arg] for arg in self.argnames if arg in self.input_dict.keys()]\n",
        "        self.outputs    = self.forward(*args, **kwargs)\n",
        "\n",
        "        ## Make sure outputs are tensors and tie back to this layer\n",
        "        list_outs = isinstance(self.outputs, list) or isinstance(self.outputs, tuple)\n",
        "        if not list_outs:\n",
        "            self.outputs = [self.outputs]\n",
        "        self.outputs = [Tensor(out) for out in self.outputs]\n",
        "        for out in self.outputs: \n",
        "            out.backward = self.backward\n",
        "\n",
        "        # print(self.__class__.__name__.ljust(24), [v.shape for v in self.inputs], '->', [v.shape for v in self.outputs])\n",
        "            \n",
        "        ## And then finally, it returns the output, thereby wrapping the forward\n",
        "        return self.outputs if list_outs else self.outputs[0]\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns a list of parameters\"\"\"\n",
        "        return ()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        \"\"\"Pass inputs through function. Can store inputs and outputs as instance variables\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Returns local gradient of layer output w.r.t. input\"\"\"\n",
        "        pass\n",
        "\n",
        "    def weight_gradients(self):\n",
        "        \"\"\"Returns local gradient of layer output w.r.t. weights\"\"\"\n",
        "        return []\n",
        "    \n",
        "    @abstractmethod\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        \"\"\"\n",
        "        Propagate upstream gradient backwards by composing with local gradient\n",
        "        \n",
        "        SCAFFOLD: \n",
        "\n",
        "        Differentiate with respect to layer parameters:\n",
        "            For every param-gradient pair\n",
        "            - If all Tensors or this tensor do not require gradients, then skip\n",
        "            - Otherwise, compose upstream and local gradient\n",
        "        \n",
        "        Differentiate with respect to layer input:\n",
        "            For every input-gradient pair\n",
        "            - If all Tensors or this tensor do not require gradients, then skip\n",
        "            - Otherwise, compose upstream and local gradient\n",
        "\n",
        "        Usefulseful print boilerplate...: \n",
        "            # print(f'Diffing w.r.t. \"{k}\": local = {g.shape} and upstream = {grad.shape}')\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n50HNq3ZuCO"
      },
      "source": [
        "### Loss\n",
        "**[TODO]:** Implement the forward pass in `forward()`.\n",
        "- The forward pass should just give the mean squared error between `y_pred` and `y_true`.\n",
        "\n",
        "**[TODO]:** Implement the backward pass in `backward()`.\n",
        "- This should take advantage of the layer inputs as well as the gradients computed with respect to them.\n",
        "- Feel free to only work with the input gradients, since this loss layer does not have any parameters.\n",
        "\n",
        "**[TODO]:** Calculate and return `input_gradients()`:\n",
        "- You want to calculate the gradients which flow to the inputs: `y_pred` and `y_true`\n",
        "- Note that we don't want to \"train\" `y_true`, so you can just return 0 for the grads for `y_true`\n",
        "- Return the partial derivative of mean squared error w.r.t. `y_pred`, and 0.\n",
        "\n",
        "Note that we don't need to implement `weight_gradients()` here because MSELoss doesn't have weights!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {
        "id": "1GG1E4VaC9hs"
      },
      "outputs": [],
      "source": [
        "class MSELoss(Diffable):\n",
        "\n",
        "    \"\"\"\n",
        "        Calculates mean squared error loss and gradient w.r.t. inputs.\n",
        "        Subclasses Diffable.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"Mean squared error forward pass!\"\"\"\n",
        "        # TODO: Compute and return the MSE given predicted and actual labels\n",
        "        mse = np.square(y_true-y_pred).mean()\n",
        "        return mse\n",
        "\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Mean squared error backpropagation!\"\"\"\n",
        "        # TODO: Compute and return the gradients\n",
        "        differences = self.inputs[1] - self.inputs[0]\n",
        "        return [-2*differences/self.inputs[0].shape[0], 0]\n",
        "\n",
        "  \n",
        "        \n",
        "\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        \"\"\"Mean squared error backpropagation!\"\"\"        \n",
        "        ## TODO: Differentiate with respect to layer inputs        \n",
        "        ## For each input value and input gradient\n",
        "            ## Compose the upstream gradient with this input's gradient\n",
        "            ## Set the gradient of the tensor to the composed gradient as necessary\n",
        "            ## Pass the composed gradient backward through structure\n",
        "        new_grad = self.input_gradients()[0] * grad\n",
        "        if(self.inputs[0].requires_grad):\n",
        "          self.inputs[0].grad= new_grad\n",
        "          self.inputs[0].backward(new_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tdEZezDRLEPk"
      },
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHqCz0nKsGB"
      },
      "source": [
        "And here are some sanity checks you can run to make sure that your code is working as intended. In the first check, the outputs should match. In the second, they should be within the specified range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45Y7ngPUKehB",
        "outputId": "554a334f-5f21-4d8a-8717-6ce35354840a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3733.0652, grad_fn=<MseLossBackward0>)\n",
            "3733.0651504088946\n"
          ]
        }
      ],
      "source": [
        "class con: \n",
        "    ## Control set using default PyTorch\n",
        "    ytrue = torch.Tensor(Y0)\n",
        "    ypred = torch_model(torch.Tensor(X0))\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "class exp: \n",
        "    ## Experimental set using your own implementation\n",
        "    ytrue = Tensor(Y0)\n",
        "    ypred = Tensor(con.ypred.detach().numpy())\n",
        "    loss_fn = MSELoss()\n",
        "\n",
        "def ypred_to_loss(ns):\n",
        "    ## Compute loss using the control and experimental namespaces\n",
        "    ns.loss = ns.loss_fn(ns.ypred, ns.ytrue)\n",
        "    return ns.loss\n",
        "\n",
        "## Sanity Check 1: Make sure that the forward pass is the same (i.e. your implementation matches the control)\n",
        "print(ypred_to_loss(con))\n",
        "print(ypred_to_loss(exp))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9tUAGJkg6I9",
        "outputId": "b2662d1d-25e2-42a7-d0be-3e341cbcf26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(353, 1)\n",
            "(353, 1)\n",
            "Maximum difference 3.630311229407823e-08 should be less than 0.00001\n"
          ]
        }
      ],
      "source": [
        "## Sanity Check 2: Make sure that the backwards pass is the same\n",
        "\n",
        "con.ypred = con.ypred.detach()\n",
        "con.ypred.requires_grad = True\n",
        "# print(\"Before running backwards:\\n\", con.ypred.grad)\n",
        "ypred_to_loss(con)\n",
        "con.loss.backward()\n",
        "# print(\"After running backwards:\\n\", con.ypred.grad)\n",
        "\n",
        "exp.ypred.grad = None\n",
        "# print(\"Before running backwards:\\n\", np.round(exp.ypred.grad, 4))\n",
        "ypred_to_loss(exp)\n",
        "exp.loss.backward()\n",
        "# print(\"After running backwards:\\n\", np.round(exp.ypred.grad, 4))\n",
        "print(exp.ypred.grad.shape)\n",
        "print(con.ypred.grad.detach().numpy().shape)\n",
        "max_diff = np.max(exp.ypred.grad - con.ypred.grad.detach().numpy())\n",
        "print(f\"Maximum difference {max_diff} should be less than 0.00001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inej4Q37gqN8"
      },
      "source": [
        "### Linear Layer\n",
        "Next, a linear layer!\n",
        "\n",
        "**[TODO]:** Implement the `forward()` pass of a linear layer. \n",
        "\n",
        "**[TODO]:** Calculate (manually) the weight gradients.\n",
        "- Manually differentiate the Dense layer with respect to weights and biases.\n",
        "- Return weight gradient, then bias gradient in that order\n",
        "- HINT: How is differentiating with matrix variables similar to and different from normal differentiation?\n",
        "\n",
        "**[TODO]:** Initalize weights and biases in `_initialize_weight()`.\n",
        "- In a linear layer, we have 2 parameters: weights and biases. \n",
        "- Return two NumPy arrays of the correct shapes according according to the function's arguments. \n",
        "- Return weights and biases, in that order.\n",
        "\n",
        "**[TODO]:** Implement the backward function.\n",
        "- Feel free to only work with the weight gradients, since we do not yet need to support multilayered networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50FnJezypdpt",
        "outputId": "f2c987ef-343d-4ee9-ecc3-0c9a3a6d1926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(353, 1)\n",
            "(353, 1)\n",
            "Maximum difference 8.47544684434709e-09 should be less than 0.00001\n",
            "\n",
            "Losses: Control 28772.294921875 vs Experimental 28772.295314060244\n",
            "\n",
            "Control Params:\n",
            "Parameter containing:\n",
            "tensor([[ 0.2894,  0.1591,  0.1821,  0.0823,  0.1595,  0.0495, -0.1101, -0.0031,\n",
            "          0.0245, -0.2570]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0799], requires_grad=True)\n",
            "\n",
            "Experimental Params:\n",
            "[[ 0.28938657  0.15906665  0.18206444  0.08228967  0.15953891  0.04954562\n",
            "  -0.11012045 -0.00310192  0.02452306 -0.25704113]]\n",
            "[-0.07992715]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Linear(Diffable):\n",
        "\n",
        "    \"\"\"\n",
        "        Standard linear/dense layer.\n",
        "        Subclasses Diffable.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
        "        self.in_features = in_features\n",
        "        self.w, self.b = self.__class__._initialize_weight(in_features, out_features)\n",
        "    \n",
        "    def parameters(self):\n",
        "        return self.w, self.b\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass for a dense layer! Refer to lecture slides for how this is computed.\"\"\"\n",
        "        # TODO: implement the forward pass and return the outputs\n",
        "        w = self.w.reshape(10,1)\n",
        "        return np.matmul(inputs,w) + self.b\n",
        "\n",
        "    def weight_gradients(self):\n",
        "        \"\"\"Calculating the gradients of the weights and biases!\"\"\"\n",
        "        # TODO: Implement calculation of gradients\\\n",
        "        weight = np.sum(self.inputs[0],axis=0)\n",
        "        bias = np.zeros(0)\n",
        "        return (weight,bias)\n",
        "\n",
        "\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Calculate the gradients of the inputs! (Not necessary for HW1)\"\"\"\n",
        "        return (self.w,)\n",
        "\n",
        "    @staticmethod\n",
        "    def _initialize_weight(input_size, output_size):\n",
        "        \"\"\"\n",
        "        Initializes the values of the weights and biases. You can assume that \n",
        "        bias is a zero-vector and weight is normally-distributed.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement default assumption: zero-init for bias, normal distribution for weights\n",
        "        ## Must return tensors for tracking purposes.\n",
        "        b = np.zeros((output_size))\n",
        "        mu, sigma = 0, 1\n",
        "        w = np.random.normal(mu, sigma, input_size*output_size).reshape((input_size,output_size))\n",
        "        return (w,b)\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        ## For every weight/bias and weight/bias gradient\n",
        "            ## Compose the upstream gradient with this weight's/bias's gradient\n",
        "            ## Set the gradient of the tensor to the composed gradient if necessary\n",
        "            ## Backpropagate the composed gradient through the structure\n",
        "        new_grad = grad * self.weight_gradients()[0]\n",
        "        print(self.weight_gradients()[0].shape)\n",
        "        print(self.weight_gradients()[0].shape)\n",
        "        if(self.parameters()[0].requires_grad):\n",
        "          self.parameters()[0].grad= new_grad\n",
        "          self.parameters()[0].backward(new_grad)\n",
        "        if(self.parameters()[1].requires_grad):\n",
        "          self.parameters()[1].grad= new_grad\n",
        "          self.parameters()[1].backward(new_grad)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class con:\n",
        "    ## Control set using regular pytorch\n",
        "    X0 = torch.Tensor(X0)\n",
        "    Y0 = torch.Tensor(Y0)\n",
        "    X0.requires_grad = True\n",
        "    Y0.requires_grad = True\n",
        "    dense = nn.Linear(10, 1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "class exp:\n",
        "    ## Experimental set using your own implementation\n",
        "    X0 = Tensor(X0)\n",
        "    Y0 = Tensor(Y0)\n",
        "    dense = Linear(10, 1)\n",
        "    dense.w, dense.b = [Tensor(p.detach().numpy()) for p in con.dense.parameters()]\n",
        "    loss_fn = MSELoss()\n",
        "\n",
        "def x_to_loss(ns):\n",
        "    ns.ypred = ns.dense(ns.X0)\n",
        "    ns.loss  = ns.loss_fn(ns.ypred, ns.Y0)\n",
        "    return ns.loss\n",
        "\n",
        "x_to_loss(con)\n",
        "x_to_loss(exp)\n",
        "\n",
        "## Sanity Check 1: Make sure that the forward pass is the same\n",
        "# print(con.ypred)\n",
        "# print(exp.ypred)\n",
        "print(con.ypred.detach().numpy().shape)\n",
        "print(exp.ypred.shape)\n",
        "print(f\"Maximum difference {np.max(con.ypred.detach().numpy() - exp.ypred)} should be less than 0.00001\\n\")\n",
        "\n",
        "print(f\"Losses: Control {con.loss} vs Experimental {exp.loss}\")\n",
        "\n",
        "print('\\nControl Params:',      *list(con.dense.parameters()), sep='\\n')\n",
        "print('\\nExperimental Params:', *list(exp.dense.parameters()), sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v6oUfSJxidG",
        "outputId": "8f017d90-3f35-4261-a66e-3e77523a4def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After running backwards on weights:\n",
            "[tensor([[-1.5537, -0.0819, -3.9114, -3.1331, -1.6591, -1.4872,  2.7267, -3.2806,\n",
            "         -3.9166, -2.6445]]), tensor([-303.1168])]\n",
            "(10,)\n",
            "(10,)\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "After running backwards on weights:\n",
            "[Tensor([[-0.26769171,  0.0260229 ,  0.18128938, ...,  0.01730089,\n",
            "          0.17060693,  0.19795376],\n",
            "        [-0.13307327,  0.01293634,  0.09012147, ...,  0.00860051,\n",
            "          0.08481107,  0.09840556],\n",
            "        [-0.24995409,  0.02429858,  0.1692769 , ...,  0.01615451,\n",
            "          0.15930228,  0.18483707],\n",
            "        ...,\n",
            "        [-0.43081752,  0.04188072,  0.2917634 , ...,  0.0278437 ,\n",
            "          0.27457127,  0.3185827 ],\n",
            "        [-0.12602922,  0.01225158,  0.08535102, ...,  0.00814526,\n",
            "          0.08032172,  0.0931966 ],\n",
            "        [-0.13664282,  0.01328335,  0.09253889, ...,  0.00883121,\n",
            "          0.08708604,  0.10104519]]), Tensor([[-0.26769171,  0.0260229 ,  0.18128938, ...,  0.01730089,\n",
            "          0.17060693,  0.19795376],\n",
            "        [-0.13307327,  0.01293634,  0.09012147, ...,  0.00860051,\n",
            "          0.08481107,  0.09840556],\n",
            "        [-0.24995409,  0.02429858,  0.1692769 , ...,  0.01615451,\n",
            "          0.15930228,  0.18483707],\n",
            "        ...,\n",
            "        [-0.43081752,  0.04188072,  0.2917634 , ...,  0.0278437 ,\n",
            "          0.27457127,  0.3185827 ],\n",
            "        [-0.12602922,  0.01225158,  0.08535102, ...,  0.00814526,\n",
            "          0.08032172,  0.0931966 ],\n",
            "        [-0.13664282,  0.01328335,  0.09253889, ...,  0.00883121,\n",
            "          0.08708604,  0.10104519]])]\n"
          ]
        }
      ],
      "source": [
        "## Sanity Check 2: Make sure that the backwards pass is the same\n",
        "\n",
        "con.X0 = con.X0.detach()\n",
        "con.Y0 = con.Y0.detach()\n",
        "for p in con.dense.parameters():\n",
        "    if p.grad is None: continue\n",
        "    p.grad.detach_()\n",
        "    p.grad = None\n",
        "\n",
        "x_to_loss(con).backward()\n",
        "print(\"After running backwards on weights:\")  \n",
        "print([p.grad for p in con.dense.parameters()])\n",
        "\n",
        "for p in exp.dense.parameters(): p.grad = None\n",
        "x_to_loss(exp).backward()\n",
        "\n",
        "print(\"\\n\" + \"*\" * 100 + \"\\n\")\n",
        "print(\"After running backwards on weights:\")  \n",
        "print([p.grad for p in exp.dense.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGhTAMgFe_i"
      },
      "source": [
        "## Optimizing With The Gradients\n",
        "\n",
        "To use the gradients we calculated previously, we need an optimizer. The optimizer allows us to update our weights and bias. A simple approach could be to simply subtract the gradient from the weights and bias. In doing so, we follow the gradient in its opposite direction, minimizing loss. This is what is called gradient descent. \n",
        "\n",
        "However, simply subtracting the gradients from the weights could result in the weights changing wildly between each sample, making training longer. To prevent this, we use a learning rate. The learning rate is a hyperparameter that specifies how much a single step updates weights. A smaller learning rate means that the gradients have less of an impact on the weights, and vice versa.\n",
        "\n",
        "Of course, this is just one (simple) approach. In a later lab, you'll learn about other optimizers, such as Adam and RMSProp.\n",
        "\n",
        "**[TODO]:** Implement stochastic gradient descent for each parameter using the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 501,
      "metadata": {
        "id": "J5s6o4_WGXhe"
      },
      "outputs": [],
      "source": [
        "class SGD: \n",
        "    \"\"\"\n",
        "        Performs stochastic gradident descent with the specified learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr, *args, **kwargs):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "            Reset the gradients.\n",
        "        \"\"\"\n",
        "        pass\n",
        "            \n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "            Update paramaters by subtracting the gradient multiplied by the learning rate.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement stochastic grad descent for each parameter\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9m63xegnLnj"
      },
      "source": [
        "Below, you'll use your new implementations to optimize for linear regression manually. FakeTorchModule will also be provided to make some of the mimicking process easier. \n",
        "\n",
        "**[TODO]:** Complete the model and compare this model's performance to the previous `LinearRegression` model -- they should have a similar loss after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "M6-a0DfvFlnN",
        "outputId": "a4c2f802-99aa-4224-952a-c10510a955b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-502-8bc524902c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManualLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m model.train_test(\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-490-59d88933ad74>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(self, train_data, test_data, epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mall_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mall_stats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtest_stats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-490-59d88933ad74>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m## Erase the gradient history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;31m## Do a forward pass on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ManualLinearRegression' object has no attribute 'optimizer'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FakeTorchModule:\n",
        "    \"\"\"\n",
        "        Needed so that we can do manual linear regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = \"\"\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def to(self, device):\n",
        "        return self\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for k,v in self.__dict__.items():\n",
        "            params += getattr(v, 'parameters', lambda: [])()\n",
        "        return params\n",
        "\n",
        "    def train(self):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = getattr(p, 'required_grad', p.requires_grad)\n",
        "    \n",
        "    def eval(self):\n",
        "        for p in self.parameters():\n",
        "            p.required_grad = p.requires_grad\n",
        "            p.requires_grad = False\n",
        "\n",
        "class ManualRegression(FakeTorchModule):\n",
        "    \"\"\"\n",
        "        Allows us to use our custom Linear layer and SGD optimizer.\n",
        "        Subclasses FakeTorchModule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__()\n",
        "        ## TODO: Incorporate your custom components in the initialization pipeline. \n",
        "        self.set_learning_rate()\n",
        "\n",
        "    def set_learning_rate(self, learning_rate=0.001):\n",
        "        ## TODO: Use your new SGD component and make changes as appropriate.\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## TODO: Implement the forward function as appropriate. Make changes as necessary\n",
        "        return x\n",
        "\n",
        "class TrainTest2(TrainTest):\n",
        "    # no_grad = torch.no_grad\n",
        "    no_grad = Tensor.no_grad\n",
        "\n",
        "class ManualLinearRegression(ManualRegression, TrainTest2):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__(input_dims, output_dims)\n",
        "        ## TODO: Implement the subclass as appropriate with your own implementations\n",
        "\n",
        "## Train the manual linear regression model\n",
        "model = ManualLinearRegression(10, 1)\n",
        "model.set_learning_rate(0.2)\n",
        "model.train_test(\n",
        "    [[Tensor(X0), Tensor(Y0)]], \n",
        "    [[Tensor(X1), Tensor(Y1)]],\n",
        "    epochs=200\n",
        ");\n",
        "## TODO: Compare this model's performance to the first linear regression model -- they should be similar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhxMFrlPOX2J"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DGrd50k0J5"
      },
      "source": [
        "## Wrapping Up\n",
        "\n",
        "Congratulations, you've finished this assignment! You should now have a better understand of linear regression, loss functions, and optimizers/gradient descent. This assignment provides the foundation for Homework 2, so feel free to come back or read it over again to get a solid understanding.\n",
        "\n",
        "Be sure to submit your finished notebook (follow the guidelines on the handout)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "759be6693a164ddeab1e231298c2a01a8302a7c7dfd4e560844dbce42a896f34"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}